{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import wrangle\n",
    "from pydataset import data\n",
    "\n",
    "import pyspark\n",
    "import pyspark.ml\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wrangle.py] reading case.csv\n",
      "[wrangle.py] handling data types\n",
      "[wrangle.py] parsing dates\n",
      "[wrangle.py] adding features\n",
      "[wrangle.py] joining departments\n"
     ]
    }
   ],
   "source": [
    "df = wrangle.wrangle_311(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the .randomSplit method to split the 311 data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(df: pyspark.sql.DataFrame):\n",
    "    return df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(672840, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168864, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case_id',\n",
       " 'case_opened_date',\n",
       " 'case_closed_date',\n",
       " 'case_due_date',\n",
       " 'case_late',\n",
       " 'num_days_late',\n",
       " 'case_closed',\n",
       " 'service_request_type',\n",
       " 'SLA_days',\n",
       " 'case_status',\n",
       " 'source_id',\n",
       " 'request_address',\n",
       " 'council_district',\n",
       " 'num_weeks_late',\n",
       " 'zipcode',\n",
       " 'case_age',\n",
       " 'days_to_closed',\n",
       " 'case_lifetime',\n",
       " 'department',\n",
       " 'dept_subject_to_SLA']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a classification model to predict whether a case will be late or not (i.e. predict case_late). Experiment with different combinations of features and different classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+------------------+-----------+---------+--------------------+----------------+-------------------+-------+--------+--------------+-------------+--------------------+-------------------+-------------+-----+\n",
      "|   case_id|   case_opened_date|   case_closed_date|      case_due_date|case_late|      num_days_late|case_closed|service_request_type|          SLA_days|case_status|source_id|     request_address|council_district|     num_weeks_late|zipcode|case_age|days_to_closed|case_lifetime|          department|dept_subject_to_SLA|     features|label|\n",
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+------------------+-----------+---------+--------------------+----------------+-------------------+-------+--------+--------------+-------------+--------------------+-------------------+-------------+-----+\n",
      "|1014127332|2018-01-01 00:42:00|2018-01-01 12:29:00|2020-09-26 00:42:00|    false| -998.5087616000001|       true|        Stray Animal|             999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|             005|       -142.6441088|  78207|     219|             0|            0|Animal Care Services|               true|(7,[2],[1.0])|  0.0|\n",
      "|1014127336|2018-01-01 01:34:00|2018-01-01 13:29:00|2018-01-01 04:34:00|     true|0.37216435200000003|       true|Animal Cruelty(Cr...|             0.125|     Closed| svcCRMSS|734  CLEARVIEW DR...|             007|        0.053166336|  78228|     219|             0|            0|Animal Care Services|               true|(7,[2],[1.0])|  1.0|\n",
      "|1014127337|2018-01-01 06:28:00|2018-01-01 14:38:00|2018-01-31 08:30:00|    false|       -29.74398148|       true|Traffic Signal Op...|       30.08446759|     Closed| svcCRMSS|BANDERA RD and BR...|             007| -4.249140211428571|       |     219|             0|            0|Trans & Cap Impro...|               true|(7,[3],[1.0])|  0.0|\n",
      "|1014127338|2018-01-01 06:57:00|2018-01-02 15:32:00|2018-01-17 08:30:00|    false|       -14.70673611|       true|Front Or Side Yar...|       16.06429398|     Closed| svcCRMSS|10133  FIGARO CAN...|             004|-2.1009623014285714|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(7,[0],[1.0])|  0.0|\n",
      "|1014127339|2018-01-01 06:58:00|2018-01-02 15:32:00|2018-01-17 08:30:00|    false|       -14.70662037|       true|Front Or Side Yar...|16.063796300000003|     Closed| svcCRMSS|10133  FIGARO CAN...|             004| -2.100945767142857|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(7,[0],[1.0])|  0.0|\n",
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+------------------+-----------+---------+--------------------+----------------+-------------------+-------+--------+--------------+-------------+--------------------+-------------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = pyspark.ml.feature.RFormula(formula=\"case_late ~ department\").fit(train)\n",
    "rf\n",
    "\n",
    "rf.transform(train).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     features|label|\n",
      "+-------------+-----+\n",
      "|(7,[2],[1.0])|  0.0|\n",
      "|(7,[2],[1.0])|  1.0|\n",
      "|(7,[3],[1.0])|  0.0|\n",
      "|(7,[0],[1.0])|  0.0|\n",
      "|(7,[0],[1.0])|  0.0|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_input = rf.transform(train).select('features', 'label')\n",
    "train_input.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_559cfc766544"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = pyspark.ml.classification.LogisticRegression()\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+--------------------+--------------------+----------+\n",
      "|     features|label|       rawPrediction|         probability|prediction|\n",
      "+-------------+-----+--------------------+--------------------+----------+\n",
      "|(7,[2],[1.0])|  0.0|[1.39705845806300...|[0.80171669533291...|       0.0|\n",
      "|(7,[2],[1.0])|  1.0|[1.39705845806300...|[0.80171669533291...|       0.0|\n",
      "|(7,[3],[1.0])|  0.0|[2.80807436948043...|[0.94311059144247...|       0.0|\n",
      "|(7,[0],[1.0])|  0.0|[2.41904045591730...|[0.91826775765355...|       0.0|\n",
      "|(7,[0],[1.0])|  0.0|[2.41904045591730...|[0.91826775765355...|       0.0|\n",
      "+-------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_fit = lr.fit(train_input)\n",
    "lr_fit.transform(train_input).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'areaUnderROC',\n",
       " 'fMeasureByLabel',\n",
       " 'fMeasureByThreshold',\n",
       " 'falsePositiveRateByLabel',\n",
       " 'featuresCol',\n",
       " 'labelCol',\n",
       " 'labels',\n",
       " 'objectiveHistory',\n",
       " 'pr',\n",
       " 'precisionByLabel',\n",
       " 'precisionByThreshold',\n",
       " 'predictionCol',\n",
       " 'predictions',\n",
       " 'probabilityCol',\n",
       " 'recallByLabel',\n",
       " 'recallByThreshold',\n",
       " 'roc',\n",
       " 'totalIterations',\n",
       " 'truePositiveRateByLabel',\n",
       " 'weightedFMeasure',\n",
       " 'weightedFalsePositiveRate',\n",
       " 'weightedPrecision',\n",
       " 'weightedRecall',\n",
       " 'weightedTruePositiveRate']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(lr_fit.summary) if not x.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6225099331762476"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# area under TPR (recall) vs FPR (FP / (FP + TN)) curve\n",
    "# https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "lr_fit.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6276238295880469"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = pyspark.ml.evaluation.BinaryClassificationEvaluator()\n",
    "test_auc = evaluator.evaluate(lr_fit.transform(rf.transform(test)))\n",
    "test_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a regression model to predict how many days late a case will be (i.e. predict num_days_late). Experiment with different combinations of features and different regression algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+------------------+-----------+---------+--------------------+----------------+-------------------+-------+--------+--------------+-------------+--------------------+-------------------+-------------+-------------------+\n",
      "|   case_id|   case_opened_date|   case_closed_date|      case_due_date|case_late|      num_days_late|case_closed|service_request_type|          SLA_days|case_status|source_id|     request_address|council_district|     num_weeks_late|zipcode|case_age|days_to_closed|case_lifetime|          department|dept_subject_to_SLA|     features|              label|\n",
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+------------------+-----------+---------+--------------------+----------------+-------------------+-------+--------+--------------+-------------+--------------------+-------------------+-------------+-------------------+\n",
      "|1014127332|2018-01-01 00:42:00|2018-01-01 12:29:00|2020-09-26 00:42:00|    false| -998.5087616000001|       true|        Stray Animal|             999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|             005|       -142.6441088|  78207|     219|             0|            0|Animal Care Services|               true|(7,[2],[1.0])| -998.5087616000001|\n",
      "|1014127336|2018-01-01 01:34:00|2018-01-01 13:29:00|2018-01-01 04:34:00|     true|0.37216435200000003|       true|Animal Cruelty(Cr...|             0.125|     Closed| svcCRMSS|734  CLEARVIEW DR...|             007|        0.053166336|  78228|     219|             0|            0|Animal Care Services|               true|(7,[2],[1.0])|0.37216435200000003|\n",
      "|1014127337|2018-01-01 06:28:00|2018-01-01 14:38:00|2018-01-31 08:30:00|    false|       -29.74398148|       true|Traffic Signal Op...|       30.08446759|     Closed| svcCRMSS|BANDERA RD and BR...|             007| -4.249140211428571|       |     219|             0|            0|Trans & Cap Impro...|               true|(7,[3],[1.0])|       -29.74398148|\n",
      "|1014127338|2018-01-01 06:57:00|2018-01-02 15:32:00|2018-01-17 08:30:00|    false|       -14.70673611|       true|Front Or Side Yar...|       16.06429398|     Closed| svcCRMSS|10133  FIGARO CAN...|             004|-2.1009623014285714|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(7,[0],[1.0])|       -14.70673611|\n",
      "|1014127339|2018-01-01 06:58:00|2018-01-02 15:32:00|2018-01-17 08:30:00|    false|       -14.70662037|       true|Front Or Side Yar...|16.063796300000003|     Closed| svcCRMSS|10133  FIGARO CAN...|             004| -2.100945767142857|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(7,[0],[1.0])|       -14.70662037|\n",
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+------------------+-----------+---------+--------------------+----------------+-------------------+-------+--------+--------------+-------------+--------------------+-------------------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = pyspark.ml.feature.RFormula(formula=\"num_days_late ~ department\").fit(train)\n",
    "rf\n",
    "\n",
    "rf.transform(train).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_971c5431db60"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = pyspark.ml.regression.LinearRegression()\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+--------------------+\n",
      "|     features|label|          prediction|\n",
      "+-------------+-----+--------------------+\n",
      "|(7,[2],[1.0])|  0.0| 0.19828325260405505|\n",
      "|(7,[2],[1.0])|  1.0| 0.19828325260405505|\n",
      "|(7,[3],[1.0])|  0.0|0.056889420700777786|\n",
      "|(7,[0],[1.0])|  0.0| 0.08173245392037004|\n",
      "|(7,[0],[1.0])|  0.0| 0.08173245392037004|\n",
      "+-------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_fit = lr.fit(train_input)\n",
    "lr_fit.transform(train_input).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+-----------+-----------+---------+--------------------+----------------+--------------------+-------+--------+--------------+-------------+--------------------+-------------------+-------------+-------------------+--------------------+\n",
      "|   case_id|   case_opened_date|   case_closed_date|      case_due_date|case_late|      num_days_late|case_closed|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|      num_weeks_late|zipcode|case_age|days_to_closed|case_lifetime|          department|dept_subject_to_SLA|     features|              label|          prediction|\n",
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+-----------+-----------+---------+--------------------+----------------+--------------------+-------+--------+--------------+-------------+--------------------+-------------------+-------------+-------------------+--------------------+\n",
      "|1014127333|2018-01-01 00:46:00|2018-01-03 08:11:00|2018-01-05 08:30:00|    false|-2.0126041669999997|       true|Removal Of Obstru...|4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|             003|-0.28751488099999994|  78223|     219|             2|            2|Trans & Cap Impro...|               true|(7,[3],[1.0])|-2.0126041669999997|0.056889420700777786|\n",
      "|1014127334|2018-01-01 00:48:00|2018-01-02 07:57:00|2018-01-05 08:30:00|    false|       -3.022337963|       true|Removal Of Obstru...|4.320729167|     Closed| svcCRMSS|102  PALFREY ST W...|             003|-0.43176256614285713|  78223|     219|             1|            1|Trans & Cap Impro...|               true|(7,[3],[1.0])|       -3.022337963|0.056889420700777786|\n",
      "|1014127335|2018-01-01 01:29:00|2018-01-02 08:13:00|2018-01-17 08:30:00|    false|       -15.01148148|       true|Front Or Side Yar...|16.29188657|     Closed| svcCRMSS|114  LA GARDE ST,...|             003| -2.1444973542857144|  78223|     219|             1|            1|DSD/Code Enforcement|               true|(7,[0],[1.0])|       -15.01148148| 0.08173245392037004|\n",
      "|1014127340|2018-01-01 06:58:00|2018-01-02 15:32:00|2018-01-17 08:30:00|    false|       -14.70662037|       true|Right Of Way/Side...|16.06333333|     Closed| svcCRMSS|10133  FIGARO CAN...|             004|  -2.100945767142857|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(7,[0],[1.0])|       -14.70662037| 0.08173245392037004|\n",
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+-----------+-----------+---------+--------------------+----------------+--------------------+-------+--------+--------------+-------------+--------------------+-------------------+-------------+-------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input = rf.transform(test)\n",
    "lr_fit.transform(test_input).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o634.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 198.0 failed 1 times, most recent failure: Lost task 13.0 in stage 198.0 (TID 1508, localhost, executor driver): scala.MatchError: [-2.974418726197174E-13,null] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\r\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator$$anonfun$1.apply(RegressionEvaluator.scala:83)\r\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator$$anonfun$1.apply(RegressionEvaluator.scala:83)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:57)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:54)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)\r\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: scala.MatchError: [-2.974418726197174E-13,null] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\r\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator$$anonfun$1.apply(RegressionEvaluator.scala:83)\r\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator$$anonfun$1.apply(RegressionEvaluator.scala:83)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6c0e48302b6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegressionEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrmse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \"\"\"\n\u001b[0;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o634.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 198.0 failed 1 times, most recent failure: Lost task 13.0 in stage 198.0 (TID 1508, localhost, executor driver): scala.MatchError: [-2.974418726197174E-13,null] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\r\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator$$anonfun$1.apply(RegressionEvaluator.scala:83)\r\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator$$anonfun$1.apply(RegressionEvaluator.scala:83)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1137)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:57)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:54)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)\r\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)\r\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: scala.MatchError: [-2.974418726197174E-13,null] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\r\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator$$anonfun$1.apply(RegressionEvaluator.scala:83)\r\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator$$anonfun$1.apply(RegressionEvaluator.scala:83)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\r\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "evaluator = pyspark.ml.evaluation.RegressionEvaluator()\n",
    "rmse = evaluator.evaluate(lr_fit.transform(test_input))\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
